{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hierarchical Time Series\n",
    "\n",
    "Author: Leo Guelman\n",
    "\n",
    "\n",
    "Building a \"toy\" version of [Prophet](https://peerj.com/preprints/3190/) from scratch to accommodate for time series that follow a hierarchical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Imports](#imports1)\n",
    "* [2. Data](#data2)\n",
    "* [3. Forecasting Model](#model3)\n",
    "    * [3.1 Linear Trend Component](#trend3_1)\n",
    "    * [3.2 Seasonality Component](#seasonality3_2)\n",
    "    * [3.3 Combining Trend and Seasonality](#trend_seasonality3_3)\n",
    "* [4. Fitting all Series Independenty](#independent_fits_4)\n",
    "* [5. Comparisson with FB Prophet](#prophet_5)\n",
    "* [6. Hierarchical Approach](#hierarchical_6)\n",
    " * [6.1 The Model](#themodel_61)\n",
    " * [6.2 Stan Code and Fitting Procedure](#stanfit_62)\n",
    " * [6.3 MCMC Diagnostics](#diagnostics_63)\n",
    " * [6.4 Forecasts](#forecasts_64)\n",
    "* [7. Prediction Error](#pred_error7)\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports <a class=\"anchor\" id=\"imports1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "parameters = {'figure.figsize': (8, 4),\n",
    "              'font.size': 8, \n",
    "              'axes.labelsize': 12}\n",
    "plt.rcParams.update(parameters)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import pystan\n",
    "import multiprocessing\n",
    "import stan_utility\n",
    "import arviz as az\n",
    "\n",
    "\n",
    "from python.ts_utils import get_linear_trend_data, fourier_series, \\\n",
    "                            plot_posterior, plot_prophet, stan_model_summary, forecast, \\\n",
    "                            forecast_hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data <a class=\"anchor\" id=\"data2\"></a>\n",
    "\n",
    "For the sake of illustration, in this notebook we will focus on quarterly domestic tourism demand in the New South Wales (NSW) region in Australia. This region is further subdivided into five smaller areas of interest, referred to as zones: Metro (NSWMetro), North Coast (NSWNthCo), South Coast (NSWSthCo), South Inner (NSWSthIn), North Inner (NSWNthIn). Domestic tourism demand is measured as the number of visitor nights Australians spend away from home. The data is in the [fpp2](https://cran.r-project.org/web/packages/fpp2/index.html) R package, with slight modified as specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get data and add stronger trend for illustration \n",
    "visnights = pd.read_csv('../data/visnights_data.csv')\n",
    "visnights = visnights.assign(\n",
    "    date = lambda df: pd.to_datetime(df[['year', 'month']].assign(DAY=1))\n",
    "    )\n",
    "n_obs = visnights.shape[0]\n",
    "adj = np.logspace(0, 0.2, num=n_obs)\n",
    "visnights['NSWMetro'] = visnights['NSWMetro'].values * adj\n",
    "visnights['NSWNthCo'] = visnights['NSWNthCo'].values * adj\n",
    "visnights['NSWSthCo'] = visnights['NSWSthCo'].values * adj\n",
    "visnights['NSWSthIn'] = visnights['NSWSthIn'].values * adj\n",
    "visnights['NSWNthIn'] = visnights['NSWNthIn'].values * adj\n",
    "visnights = visnights.drop(['year', 'month'], axis=1)\n",
    "visnights = visnights.set_index('date')\n",
    "print(\"Number of observations:\", n_obs, \"\\n\")\n",
    "visnights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows that all series share common features of trend and seasonality. As expected, tourism demand exhibits yearly seasonality, and this is reflected in spikes in each series during the first quarter of the year. Also, the series shows an upward trend that is more pronounced in the last few years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series\n",
    "\n",
    "ax = visnights.plot(linewidth=2, marker='o');\n",
    "ax.set_xlabel('Date', fontsize=12);\n",
    "ax.set_ylabel('Visitor nights (millions)', fontsize=12);\n",
    "ax.set_title(\"Domestic tourism demand in the New South Wales by zone\", fontsize=12)\n",
    "ax.legend(fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forecasting Model  <a class=\"anchor\" id=\"model3\"></a>\n",
    "\n",
    "We focus here on the need to produce a large number of fortecast in an automated fashion, where the individual time series to be forecasted share some common features of trend and/or seasonality. \n",
    "\n",
    "Methods that explicitly account for the temporal dependence structure in the data, such as Automatic ARIMA or Exponential Smoothing, require a significant amount of tuning. Unless tuned properly, they produce forecast that struggle to match the characteristics of the series. We instead follow the model proposed in [Prophet](https://peerj.com/preprints/3190/), which frames the forecasting problem as a curve-fitting exercise. Specifically, we decompose a time series $y(t)$ into three main components of trend, seasonality, and holidays:\n",
    "\n",
    "$$\n",
    "y(t)= g(t) + s(t) + h(t) + e_t.\n",
    "$$\n",
    "\n",
    "Here $g(t)$ is the trend function which models non-periodic changes in the value of the time series, $s(t)$ represents periodic changes, and $h(t)$ represents the effects of holidays which occur on potentially irregular schedules over\n",
    "one or more days. The error term $e_t$ represents any idiosyncratic changes that are not\n",
    "accounted by the aforementioned components.\n",
    "\n",
    "We cover the trend and seasonality components below. Holidays follow a similar model structure as seasonal effects, and so we leave them out from this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear Trend Component  <a class=\"anchor\" id=\"trend3_1\"></a>\n",
    "\n",
    "We fit the following trend model to the data:\n",
    "\n",
    "\\begin{align*} \n",
    "y &\\sim N(\\mu_{\\text{trend}},\\sigma) \\\\ \n",
    "\\mu_{\\text{trend}} &= (k + \\mathbf{A} \\mathbf{\\delta})t + (m+\\mathbf{A} \\gamma)  \\\\\n",
    "\\gamma &= -s * \\delta \\\\\n",
    "k &\\sim N(0,5) \\\\\n",
    "m &\\sim N(0,5) \\\\\n",
    "\\delta &\\sim \\text{Laplace}(0,1) \\\\\n",
    "\\sigma &\\sim \\text{Half-normal}(0,.5),\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where trend changes are incoporated explicitly by defining a set of $s_j$,$~j=\\{1, \\ldots, n\\}$ changepoints in which the trend is supposed to change, $k$ is the base growth rate, $\\mathbf{\\delta} \\in \\mathbb{R}^n$ is a vector of rate adjustments, $m$ is an offset parameter, and $\\gamma$ is set equal to $-s * \\delta$ so the trend function continuous.\n",
    "\n",
    "The rate at any time $t$ is then the base rate $k$, plus all the rate adjustments up to that point: $k + \\sum_{j:t>s_j}\\delta_j$. This is represented in the model by incorporating a changepoint matrix $A$ in the declaration of the mean trend, $\\mu_{\\text{trend}}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "A=\n",
    "\\begin{bmatrix}\n",
    "t_1 \\geq s_1 & \\ldots & t_1 \\geq  s_n\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "t_l \\geq s_1 & \\ldots & t_l \\geq  s_n\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend model defined above can be expressed in [stan](https://mc-stan.org/) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stored in \"stan_trend.stan\"\n",
    "\n",
    "stan_model = \"\"\"\n",
    "\n",
    "data { \n",
    "\n",
    "  int N;                              // Number of time periods\n",
    "  int N_changepoints;                 // Number of changepoints\n",
    "  vector[N_changepoints] s;           // Changepoints\n",
    "  matrix[N, N_changepoints] A;        // Changepoint Matrix\n",
    "  vector[N] t;                        // Time\n",
    "  vector[N] y;                        // Time series\n",
    "  real<lower=0> tau;                  // Scale on changepoints prior\n",
    " \n",
    "} \n",
    " \n",
    "parameters { \n",
    "    \n",
    "  vector[N_changepoints] delta;      // Trend rate adjustments\n",
    "  real k;                            // Base trend growth rate\n",
    "  real m;                            // Trend offset\n",
    "  real<lower=0> sigma_trend;         // Observation noise\n",
    "\n",
    "} \n",
    " \n",
    "transformed parameters { \n",
    "  \n",
    "  vector[N_changepoints] gamma;\n",
    "  vector[N] mu_trend; \n",
    "  \n",
    "  gamma = -s .* delta;\n",
    "  mu_trend = k + A * delta .* t + (m + A * gamma);\n",
    "   \n",
    "} \n",
    " \n",
    "model { \n",
    "\n",
    "  // Priors\n",
    "  k ~ normal(0, 5);\n",
    "  m ~ normal(0, 5);\n",
    "  delta ~ double_exponential(0, tau);\n",
    "  sigma_trend ~ normal(0, 0.5);\n",
    "  \n",
    "  // Linear likelihood\n",
    "  y ~ normal(mu_trend, sigma_trend);\n",
    "\n",
    "} \n",
    "\n",
    "generated quantities { \n",
    "    \n",
    "    vector[N] y_hat;\n",
    "    for (n in 1:N)\n",
    "        y_hat[n] = normal_rng(mu_trend[n], sigma_trend);  \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposes first, we fit the trend model to a single region (North Coast). We keep the stan code on a separate file named `stan_trend.stan` and execute it with [PyStan](https://github.com/stan-dev/pystan2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot region\n",
    "NSWNthCo_df = visnights.loc[:,'NSWNthCo']\n",
    "ax = NSWNthCo_df.plot(linewidth=2, fontsize=12);\n",
    "ax.set_xlabel('Date');\n",
    "ax.set_ylabel('NSWNthCo Visitor nights (millions)');\n",
    "ax.legend(fontsize=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trend inputs\n",
    "t, s, A = get_linear_trend_data(NSWNthCo_df.values, n_changepoints = 10)\n",
    "\n",
    "# Fit linear Trend \n",
    "stan_data_dict = {'N':NSWNthCo_df.shape[0],\n",
    "                  'N_changepoints':len(s),\n",
    "                  's': s,\n",
    "                  'A': A,\n",
    "                  't': t,\n",
    "                  'y': NSWNthCo_df.values,\n",
    "                  'tau': 1  # changepoint.prior.scale (default = 0.05)\n",
    "                  }\n",
    "\n",
    "sm = pystan.StanModel('../stan/stan_trend.stan') \n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "fit1 = sm.sampling(data=stan_data_dict, iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the mean and 90% credibility intervals on the posterior mean trend $\\mu_{\\text{trend}}$ along with the actual values of the series. The credibility interval illustrate the *epistemic uncertainty* of the model. The vertical lines show the changepoints, with the number of points being specified in the `n_changepoints` argument. This returns evenly spaced changepoints over the time interval scaled to the $[0,1]$ range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(stan_fit = fit1, component = 'trend', \n",
    "               ts = NSWNthCo_df.values, s = s, ci = [0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second plot below corresponds to the mean posterior prediction $\\hat{y}$ along with the 90% credibility interval, which in addition captures the *aleatoric uncertainty* in prediction (notice the wider interval and the fact that the linear trend wiggles within each changepoint as a result of the inclusion of aleatoric uncertainty). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(stan_fit = fit1, component = 'prediction', \n",
    "               ts = NSWNthCo_df.values, s = s, ci = [0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Seasonality Component  <a class=\"anchor\" id=\"seasonality3_2\"></a>\n",
    "\n",
    "Seasonal effect are captured by Fourier series, which can approximate arbitrary smooth effects by adding up periodic function of $t$:\n",
    "\n",
    "$$\n",
    "s(t) = \\sum_{r=1}^{R} \\Bigg(a_n \\text{cos} \\big(\\frac{2\\pi rt}{P}\\Big) + b_n \\text{sin} \\big(\\frac{2\\pi rt}{P}\\Big) \\Bigg).\n",
    "$$\n",
    "\n",
    "The seasonality model follows a simple specification:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*} \n",
    "y &\\sim N(\\mu_{\\text{seasonality}},\\sigma) \\\\ \n",
    "\\mu_{\\text{seasonality}} &= \\mathbf{X} \\mathbf{\\beta} \\\\\n",
    "\\mathbf{\\beta} &\\sim N(0,\\sigma_{\\beta}) \\\\\n",
    "\\sigma &\\sim \\text{Half-normal}(0,.5),\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\mathbf{\\beta} = [a_1, b_1, \\ldots, a_R , b_R]^\\top,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "\\text{cos} \\Big(\\frac{2 \\pi \\times 1 \\times t_1}{P}\\Big) &  \\text{sin} \\Big(\\frac{2 \\pi \\times 1 \\times t_1}{P}\\Big)  & \\ldots & \\text{cos} \\Big(\\frac{2 \\pi \\times R \\times t_1}{P}\\Big) &  \\text{sin} \\Big(\\frac{2 \\pi \\times R \\times t_1}{P}\\Big) \\\\\n",
    "\\vdots & \\vdots  & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\text{cos} \\Big(\\frac{2 \\pi \\times 1 \\times t_l}{P}\\Big) &  \\text{sin} \\Big(\\frac{2 \\pi \\times 1 \\times t_l}{P}\\Big)  & \\ldots & \\text{cos} \\Big(\\frac{2 \\pi \\times R \\times t_l}{P}\\Big) &  \\text{sin} \\Big(\\frac{2 \\pi \\times R \\times t_l}{P}\\Big)  \\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit this model is stan using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stored in \"stan_seasonality.stan\"\n",
    "\n",
    "stan_seasonality = \"\"\"\n",
    "\n",
    "data { \n",
    "\n",
    "  int N;                              // Number of time periods\n",
    "  int n_fourier;                      // Fourier series explansion\n",
    "  matrix[N, n_fourier * 2] X;         // Seasonality matrix\n",
    "  vector[N] y;                        // Time series\n",
    "  real<lower=0> sigma_beta;           // Scale on seasonality prior\n",
    "  \n",
    "} \n",
    " \n",
    "parameters { \n",
    "    \n",
    "  vector[n_fourier * 2] beta;        // Seasonality regressor coefficients\n",
    "  real<lower=0> sigma_s;             // Observation noise\n",
    "\n",
    "} \n",
    "\n",
    "transformed parameters { \n",
    "  \n",
    "  vector[N] mu_s; \n",
    "  mu_s = X * beta;\n",
    "   \n",
    "} \n",
    " \n",
    "model { \n",
    "\n",
    "  // Priors\n",
    "  beta ~ normal(0, sigma_beta);\n",
    "  sigma_s ~ normal(0, 0.5);\n",
    "  \n",
    "  // Linear likelihood\n",
    "  y ~ normal(mu_s, sigma_s);\n",
    "\n",
    "} \n",
    "\n",
    "generated quantities { \n",
    "    \n",
    "    vector[N] y_hat;\n",
    "    for (n in 1:N)\n",
    "        y_hat[n] = normal_rng(mu_s[n], sigma_s);  \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Seasonality matrix\n",
    "\n",
    "n_fourier = 10\n",
    "X = fourier_series(t, p=4, fourier_order=n_fourier) # p=4 becuase we have yearly seasonality with quarterly data\n",
    "\n",
    "stan_data_dict_s = {'N':NSWNthCo_df.shape[0],\n",
    "                    'n_fourier':n_fourier,\n",
    "                    'X':X,\n",
    "                    'y': NSWNthCo_df.values,\n",
    "                    'sigma_beta': 10,\n",
    "                   }\n",
    "\n",
    "sm = pystan.StanModel('../stan/stan_seasonality.stan') \n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "fit2 = sm.sampling(data=stan_data_dict_s, iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot below the posterior prediction for $\\mu_{\\text{seasonality}}$ along with 90% credibilty interval, and the poterior prediction $\\hat{y}$, the latter including the alteatoric uncertainty captured in $\\hat{\\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(stan_fit = fit2, component = 'seasonality', ts = NSWNthCo_df.values)\n",
    "\n",
    "plot_posterior(stan_fit = fit2, component = 'prediction', ts = NSWNthCo_df.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Combining Trend and Seasonality  <a class=\"anchor\" id=\"trend_seasonality3_3\"></a>\n",
    "\n",
    "\n",
    "Let's now combine the trend and seasonality components into a single fit. This is coded in the stan model `stan_model.stan`, included in the GitHub repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data_dict = {'N':NSWNthCo_df.shape[0],\n",
    "                  'N_changepoints':len(s),\n",
    "                  's': s,\n",
    "                  'A': A,\n",
    "                  't': t,\n",
    "                  'n_fourier':n_fourier,\n",
    "                  'X':X,\n",
    "                  'y': NSWNthCo_df.values,\n",
    "                  'tau': 1 ,        # changepoint.prior.scale\n",
    "                  'sigma_beta': 10  # seasonality.prior.scale = 10 (default),\n",
    "                  }\n",
    "\n",
    "sm = pystan.StanModel('../stan/stan_model.stan') \n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "fit3 = sm.sampling(data=stan_data_dict, iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the visualization of the trend, seasonality, and prediction posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(stan_fit = fit3, component = 'trend', ts = NSWNthCo_df)\n",
    "\n",
    "plot_posterior(stan_fit = fit3, component = 'seasonality', ts = NSWNthCo_df)\n",
    "\n",
    "plot_posterior(stan_fit = fit3, component = 'prediction', ts = NSWNthCo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting all Series Independenty  <a class=\"anchor\" id=\"independent_fits_4\"></a>\n",
    "\n",
    "We use the same framework described in [Section 3](#model3), to fit individual time series to each of the five areas within the New South Wales. In doing so, we first split each series into a train (first 66 quarterly observations), and test sets (last 10 quarterly observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = ['NSWMetro', 'NSWNthCo', 'NSWSthCo', 'NSWSthIn', 'NSWNthIn']\n",
    "n_fourier = 10\n",
    "n_changepoints = 10\n",
    "p = 4\n",
    "tau = 1  #Changepoint.prior.scale\n",
    "sigma_beta = 10 #Seasonality.prior.scale = 10 (default),\n",
    "horizon = 10 #Forecast horizon \n",
    "\n",
    "y_hat_zones = []\n",
    "\n",
    "for zone in zones:\n",
    "    \n",
    "    df = visnights.loc[:,zone]\n",
    "    df_train = df.iloc[0:(df.shape[0] - horizon)]\n",
    "    df_test =  df.iloc[(df.shape[0] - horizon):df.shape[0]]\n",
    "    \n",
    "    # Trend inputs\n",
    "    t, s, A = get_linear_trend_data(df_train.values, n_changepoints = n_changepoints)\n",
    "\n",
    "    # Seasonality inputs\n",
    "    X = fourier_series(t, p=p, fourier_order=n_fourier) \n",
    "\n",
    "    stan_data_dict = {'N':df_train.shape[0],\n",
    "                      'N_changepoints':len(s),\n",
    "                      's': s,\n",
    "                      'A': A,\n",
    "                      't': t,\n",
    "                      'n_fourier':n_fourier,\n",
    "                      'X':X,\n",
    "                      'y': df_train.values,\n",
    "                      'tau': tau ,     \n",
    "                      'sigma_beta': sigma_beta  \n",
    "                      }\n",
    "    \n",
    "    sm = pystan.StanModel('../stan/stan_model.stan') \n",
    "    multiprocessing.set_start_method(\"fork\", force=True)\n",
    "    fit4 = sm.sampling(data=stan_data_dict, iter=1000, chains=4)\n",
    "    y_hat, *_ = forecast(fit4, p = p, horizon = horizon)\n",
    "    y_hat_zones.append(y_hat)\n",
    "    plot_posterior(fit4, ts = df.values, \n",
    "                   yhat = y_hat, horizon = horizon, title = zone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparisson with FB Prophet  <a class=\"anchor\" id=\"prophet_5\"></a>\n",
    "\n",
    "We now compare our toy Prophet model with the actual [Prophet](https://peerj.com/preprints/3190/) package, and fit again each series independently using mostly default values for all parameters (we only change the number of changepoints in the trend model to be equal to the number used by our model above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "yhat_prophet_zones = []\n",
    "\n",
    "for zone in zones:\n",
    "    df = visnights.loc[:,zone]\n",
    "    df_train = df.iloc[0:(df.shape[0] - horizon)]\n",
    "    df_train = pd.DataFrame({'ds': df_train.index, 'y': df_train.values})\n",
    "    \n",
    "    fit_prophet = Prophet(n_changepoints = n_changepoints, mcmc_samples=1000)\n",
    "    fit_prophet.fit(df_train)\n",
    "    \n",
    "    df_test = pd.DataFrame({'ds': df.index})\n",
    "    \n",
    "    pred_prophet = fit_prophet.predictive_samples(df_test)\n",
    "    yhat_prophet = pred_prophet['yhat']\n",
    "    yhat_prophet_zones.append(yhat_prophet)\n",
    "    \n",
    "    plot_prophet(ts = df.values, \n",
    "                 yhat = yhat_prophet, horizon = 10, \n",
    "                 title = \"Real prophet: \"+ zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Approach  <a class=\"anchor\" id=\"hierarchical_6\"></a>\n",
    "\n",
    "As noted from the plot in [Section 2 (Data)](#data2), all series share common features of trend and seasonality. For instance, tourism demand exhibits yearly seasonality, and this is reflected in spikes in each series during the first quarter of the year. Also, the series reflect an upward trend that is more pronounced in the last few years.\n",
    "\n",
    "Given the limited data, it is reasonable to think that we should take advantage of the similar structure in the series. Specifically, instead of fitting independent models to each series, we could fit a single model to all series that includes both individual as well as shared parameters. In the context of Bayesian ineference, this is know as *partial pooling*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 The Model  <a class=\"anchor\" id=\"themodel_61\"></a>\n",
    "\n",
    "We propose the following Hierarchical model in the context of the present forecasting problem.\n",
    "\n",
    "\\begin{align*} \n",
    "y &\\sim N(\\mu_{\\text{trend}} + \\mu_{\\text{seasonality}},\\sigma) \\\\ \n",
    "\\mu_{\\text{trend}} &= (k_{\\text{zone}} + \\mathbf{A} \\mathbf{\\delta}_{\\text{zone}})t + (m+\\mathbf{A} \\gamma_{\\text{zone}})  \\\\\n",
    "\\mu_{\\text{seasonality}} &= \\mathbf{X} \\mathbf{\\beta}_{\\text{zone}} \\\\\n",
    "\\delta_z &\\sim \\text{Laplace}(\\mu_{\\delta},\\sigma_{\\delta}) \\\\\n",
    "k_z &\\sim N(\\mu_k,\\sigma_k) \\\\\n",
    "\\beta_z &\\sim N(\\mu_{\\beta},\\sigma_{\\beta}) \\\\\n",
    "\\gamma_z &= -s * \\delta_z \\\\\n",
    "m &\\sim N(0,5) \\\\\n",
    "\\sigma &\\sim \\text{Half-normal}(0,.5)\\\\\n",
    "\\mu_{\\delta} &\\sim N(0, 1)\\\\\n",
    "\\sigma_{\\delta} &\\sim N(0, 1)\\\\\n",
    "\\mu_{k} &\\sim N(0, 1)\\\\\n",
    "\\sigma_{k} &\\sim N(0,1)\\\\\n",
    "\\mu_{\\beta} &\\sim N(0, 1)\\\\\n",
    "\\sigma_{\\beta} &\\sim N(0, 1)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $z = \\{1, \\ldots, Z\\}$ indexes the zones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Stan Code and Fitting Procedure <a class=\"anchor\" id=\"stanfit_62\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stored in \"stan_model_hierarchical.stan\"\n",
    "\n",
    "stan_model_hierarchical = \"\"\"\n",
    "data { \n",
    "  int N;                              // Number of time periods\n",
    "  int N_changepoints;                 // Number of changepoints\n",
    "  vector[N_changepoints] s;           // Changepoints\n",
    "  matrix[N, N_changepoints] A;        // Changepoint Matrix\n",
    "  vector[N] t;                        // Time\n",
    "  int n_fourier;                      // Fourier series expansion\n",
    "  matrix[N, n_fourier * 2] X;         // Seasonality matrix\n",
    "  vector[N] y;                        // Time series\n",
    "  int<lower=0> J;                     // Number of groups\n",
    "  int<lower=1,upper=J> group[N];      // Group variable (integer from 1 to number of groups)\n",
    "  \n",
    "} \n",
    " \n",
    "parameters { \n",
    "    \n",
    "  vector[J] k;                       // Base trend growth rate by group\n",
    "  real m;                            // Trend offset\n",
    "  vector[N_changepoints] delta[J];   // Trend rate adjustments by group\n",
    "  vector[n_fourier * 2] beta[J];     // Seasonality regressor coefficients by group  \n",
    "  vector[n_fourier * 2] mu_beta;     // beta global mean \n",
    "  real<lower=0> sigma_beta;          // Seasonality  noise global mean \n",
    "  vector[N_changepoints] mu_delta;   // delta global mean (changepoints)\n",
    "  real<lower=0> sigma_delta;         // dela global noise (changepoints)\n",
    "  real mu_k;                         // k global mean \n",
    "  real<lower=0> sigma_k;             // k global noise\n",
    "  real<lower=0> sigma;               // Observation noise\n",
    " \n",
    "} \n",
    "\n",
    "transformed parameters { \n",
    "  \n",
    "  vector[N_changepoints] gamma[J];\n",
    "  vector[N] mu_trend; \n",
    "  vector[N] mu_s;\n",
    "  \n",
    "  for(n in 1:N){\n",
    "      gamma[group[n]] = -s .* delta[group[n]];\n",
    "      mu_trend[n] = k[group[n]] + A[n] * delta[group[n]] .* t[n] + (m + A[n] * gamma[group[n]]);\n",
    "      mu_s[n] = X[n] * beta[group[n]]; \n",
    "  } \n",
    "} \n",
    " \n",
    "model { \n",
    "\n",
    "  // Hyper-priors\n",
    "  m ~ normal(0, 5);\n",
    "  sigma ~ normal(0, 0.5);\n",
    "  mu_beta ~ normal(0, 1);\n",
    "  sigma_beta ~ normal(0, 1);\n",
    "  mu_delta ~ normal(0, 1);\n",
    "  sigma_delta ~ normal(0, 1);\n",
    "  mu_k ~ normal(0, 1);\n",
    "  sigma_k ~ normal(0,1);\n",
    "\n",
    "  // Adaptive priors\n",
    "\n",
    "  for(j in 1:J){\n",
    "    k[j] ~ normal(mu_k, sigma_k);\n",
    "    delta[j] ~ double_exponential(mu_delta, sigma_delta);\n",
    "    beta[j] ~ normal(mu_beta, sigma_beta);\n",
    "  }\n",
    "\n",
    "  // Linear likelihood\n",
    "  y ~ normal(mu_trend + mu_s, sigma);\n",
    "} \n",
    "\n",
    "generated quantities { \n",
    "    \n",
    "    vector[N] y_hat;\n",
    "    for (n in 1:N)\n",
    "        y_hat[n] = normal_rng(mu_trend[n] + mu_s[n], sigma);  \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data to fit all series simultaneously \n",
    "visnights_train = visnights.iloc[0:(visnights.shape[0] - horizon)]\n",
    "visnights_long = visnights\n",
    "visnights_long = visnights_long .reset_index()\n",
    "visnights_long = pd.melt(visnights_long, id_vars='date', \n",
    "                          value_vars= zones, \n",
    "                          var_name = 'zone', \n",
    "                          value_name = 'y')\n",
    "\n",
    "visnights_train_long = visnights_train\n",
    "visnights_train_long  = visnights_train_long.reset_index()\n",
    "visnights_train_long  = pd.melt(visnights_train_long, id_vars='date', \n",
    "                          value_vars= zones, \n",
    "                          var_name = 'zone', \n",
    "                          value_name = 'y')\n",
    "visnights_train_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model input feeds and fit\n",
    "ts = visnights_train_long['y'].values\n",
    "zone_var = visnights_train_long['zone'].values\n",
    "\n",
    "# Trend inputs\n",
    "t, s, A = get_linear_trend_data(ts, n_changepoints = n_changepoints, group_var=zone_var)\n",
    "\n",
    "# Seasonality inputs\n",
    "X = fourier_series(t, p=p, fourier_order=n_fourier) # p=4 becuase we have yearly seasonality with quarterly data\n",
    "\n",
    "stan_data_dict = {'N':visnights_train_long.shape[0],\n",
    "                  'N_changepoints':len(s),\n",
    "                  's': s,\n",
    "                  'A': A,\n",
    "                  't': t,\n",
    "                  'n_fourier':n_fourier,\n",
    "                  'X':X,\n",
    "                  'y': visnights_train_long['y'].values,\n",
    "                  'J': len(np.unique(zone_var)),\n",
    "                  'group':np.repeat(list(range(1,6)), visnights_train.shape[0]) # stan start index = 1\n",
    "                  }\n",
    "\n",
    "sm = pystan.StanModel('../stan/stan_model_hierarchical.stan') \n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "fit5 = sm.sampling(data=stan_data_dict, iter=1000, chains=4, seed=197342, control=dict(max_treedepth=15))\n",
    "\n",
    "summary_stan_fit = stan_model_summary(fit5)\n",
    "summary_stan_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 MCMC Diagnostics <a class=\"anchor\" id=\"diagnostics_63\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we want to ensure that the *Rhat* for each parameter is close to 1. Empirically, *Rhat* $> 1$ is usually indicative of problems in the fit. All parameters look good here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_hat = summary_stan_fit['Rhat']\n",
    "r_hat.plot.hist(title=\"Rhat\")\n",
    "plt.axvline(1.1, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we want to consider the effective sample size, or *n_eff*. Low samples per transition are consequences of poorly mixing Markov chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stan_fit['n_eff'].plot.hist(title=\"n_eff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic implementation of Hamiltonian Monte Carlo used in Stan has a maximum trajectory length built in to avoid infinite loops that can occur for non-identified models. For sufficiently complex models, however, Stan can saturate this threshold even if the model is identified, which limits the efficacy of the sampler. However, warnings about hitting the maximum treedepth are not as serious as warnings about divergent transitions. While divergent transitions are a validity concern, hitting the maximum treedepth is an efficiency concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_utility.utils.check_treedepth(fit5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next check the Estimated Bayesian Fraction of Missing Information (E-BFMI). E-BFMI below 0.2 indicate that the adaptation phase of the Markov Chains did not turn out well and those chains likely did not explore the posterior distribution efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_utility.utils.check_energy(fit5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can check divergences which indicate pathological neighborhoods of the posterior that the simulated Hamiltonian trajectories are not able to explore sufficiently well. No issues here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_utility.utils.check_div(fit5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Forecasts <a class=\"anchor\" id=\"forecasts_64\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, mu_trend, mu_s, train_ind = forecast_hierarchical(stan_fit=fit5, p=4, horizon=10)\n",
    "plot_posterior(fit5, ts = visnights_long['y'].values, \n",
    "               yhat = y_hat, horizon = horizon, group= visnights_long['zone'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Error <a class=\"anchor\" id=\"#pred_error7\"></a>\n",
    "\n",
    "Let's now compare the prediction error between the Hierarchical approach and the Prophet package. The hierarchical approach achieves approximately 20% lower error based on RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical\n",
    "rmse_h = (np.mean(y_hat[:,train_ind == False], axis=0) - \\\n",
    "          visnights_long['y'].values[train_ind == False])\n",
    "rmse_h = np.sqrt(sum(rmse_h ** 2)/ len(rmse_h))\n",
    "print(\"Hierarchical RMSE:\", rmse_h)\n",
    "\n",
    "#Prophet\n",
    "rmse_p = (np.mean(np.vstack(yhat_prophet_zones)[train_ind == False, :], axis=1) - \\\n",
    "          visnights_long['y'].values[train_ind == False])\n",
    "rmse_p = np.sqrt(sum(rmse_p ** 2)/ len(rmse_p))\n",
    "print(\"Prophet RMSE:\", rmse_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
